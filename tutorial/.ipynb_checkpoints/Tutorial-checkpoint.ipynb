{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Problem Requires Modern Solution\n",
    "\n",
    "If you're working on classification problem with your own dataset, or dataset that is available in their native format (jpg, bmp, etc) and use PyTorch as your main weapon, you'll most likely feel that the **DatasetFolder** or **ImageFolder** is not good enough. So does vanilla **torch.utils.data.Dataset**. This library attempts to bridge that gap to effectively Extract, Transform, and Load your data by extending **torch.utils.data.Dataset**.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "In the first step to train a classifier is to prepare the dataset. By default, PyTorch provides the abstraction of this process through **DatasetFolder** or **ImageFolder**. However they requires us to arrange our folder in this way\n",
    "##### root/class_x/xxx.ext\n",
    "##### root/class_x/xxy.ext\n",
    "##### root/class_x/xxz.ext\n",
    "##### root/class_y/123.ext\n",
    "##### root/class_y/nsdf3.ext\n",
    "##### root/class_y/asd932_.ext\n",
    "\n",
    "Most of the time, we don't have that, especially if we're using our own dataset (might be from scraping or a gift from someone). So we have to do something.\n",
    "\n",
    "The first, naive way is to move folders manually or using some scripts, could be python scripts, bash scripts or anything else you're comfortable with.\\n\\nIt's kind of cool until we    \n",
    "    1. Have terabytes of dataset    \n",
    "    2. Want to partition them into train, validation, and or test    \n",
    "    3. Want to explore/debug our dataset  \n",
    "    4. Want others to reproduce our results    \n",
    "    \n",
    "The second option is to make your own custom Dataset, subclassed from **torch.utils.data.Dataset**. This approach is much simpler and cleaner than the naive way, but still, we would have problem number 2 and 3    \n",
    "    2. Want to partition them into train, validation, and or test   \n",
    "    3. Want to explore/debug our dataset  \n",
    "    4. Want others to reproduce our results\n",
    "    \n",
    "This problem lead the design of ETL, a library that does Extract, Transform and Load on the fly. In a nutshell, Extract will read all the images from a parent directory, then partition those images into train, validation, and testing, and store them into csv files with the column of image path and encoded label. After that TransformAndLoad will ingest those samples efficiently to your classifier.\n",
    "\n",
    "By the way, the reason I use csv rather than txt is because pandas can parse CSV flawlessly. If we're using txt it's a little bit more complicated but maybe I'll add those feature in the future\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pt.1 Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torchetl.etl import Extract, TransformAndLoad\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jedi/Repo/GitHub/torchetl/data\n"
     ]
    }
   ],
   "source": [
    "parent_directory = Path.cwd().parent / 'data' \n",
    "print(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = Extract(parent_directory = parent_directory, \n",
    "              extension = 'jpg', \n",
    "              labels = ['attack', 'real'], \n",
    "              train_size = 0.8,\n",
    "              random_state = 69,\n",
    "              verbose = True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Extract in module torchetl.etl object:\n",
      "\n",
      "class Extract(torchetl.base.dataset.BaseDataset)\n",
      " |  Extract(parent_directory: str, extension: str, labels: List[str], train_size: float, random_state: int, verbose: bool) -> None\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Extract\n",
      " |      torchetl.base.dataset.BaseDataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, parent_directory: str, extension: str, labels: List[str], train_size: float, random_state: int, verbose: bool) -> None\n",
      " |      Class for creating csv files of train, validation, and test\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      parent_directory\n",
      " |              The parent_directory folder path. It is highly recommended to use Pathlib\n",
      " |      extension\n",
      " |              The extension we want to include in our search from the parent_directory directory\n",
      " |      labels\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  extract(self, file_prefix: str, save_path: str, is_random_sampling: bool)\n",
      " |      Create csv file of train, validation, and test\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      file_prefix\n",
      " |              The prefix of train, validation, and test file_prefix\n",
      " |              Have the format of file_prefix_train.csv, file_prefix_validation.csv, and test_validation.csv\n",
      " |      save_path\n",
      " |              The parent_directory folder name of file_prefix_train.csv, file_prefix_validation.csv, and test_validation.csv\n",
      " |      is_random_sampling\n",
      " |              extract train, validation, and test based on random sampling. If set to false, then stratify sampling is applied.\n",
      " |              The best practice is to use stratify sampling for classification tasks and random sampling for regression tasks\n",
      " |      Returns\n",
      " |      -------\n",
      " |      train, validation, and test csv with the following name:\n",
      " |      file_prefix_train.csv, file_prefix_validation.csv, and test_validation.csv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torchetl.base.dataset.BaseDataset:\n",
      " |  \n",
      " |  read_files(self) -> Iterable\n",
      " |      Return parent_directory directory\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      origin\n",
      " |              the path to read files\n",
      " |  \n",
      " |  read_parent_directory(self) -> str\n",
      " |      Return parent_directory directory\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  show_files(self, n: int = None) -> Sequence\n",
      " |      Show files to consume\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      number_of_files_to_show\n",
      " |              number of files to show\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      origin\n",
      " |              the path to read files\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torchetl.base.dataset.BaseDataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(combined_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we know that Extract inherits show_files method from BaseDataset. Let's print the first 5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/ori/attack/hand/112/4187.jpg'),\n",
       " PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/ori/attack/hand/112/10099.jpg'),\n",
       " PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/ori/attack/hand/112/3159.jpg'),\n",
       " PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/ori/attack/hand/112/6021.jpg'),\n",
       " PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/ori/attack/hand/112/1028.jpg')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset.show_files(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that everything went perfect, it's time to save them into our desired path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/combined')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = Path.cwd().parent / \"data\" / \"combined\"\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating whole dataset array\n",
      "Finished splitting dataset into train, validation, and test\n",
      "Finished writing combined_train.csv into /Users/jedi/Repo/GitHub/torchetl/data/combined\n",
      "Finished writing combined_validation.csv into /Users/jedi/Repo/GitHub/torchetl/data/combined\n",
      "Finished writing combined_test.csv into /Users/jedi/Repo/GitHub/torchetl/data/combined\n"
     ]
    }
   ],
   "source": [
    "combined_dataset.extract(file_prefix=\"combined\", save_path=save_path, is_random_sampling=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pt. 2 Transform and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = Path.cwd().parent / 'data' / 'combined'\n",
    "train_dataset_path = combined_dataset / 'combined_trains.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "transforms.ToPILImage(),\n",
    "transforms.RandomResizedCrop(224),\n",
    "transforms.RandomHorizontalFlip(),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jedi/Repo/GitHub/torchetl/data/combined/combined_trains.csv does not exist\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TransformAndLoad(parent_directory=parent_directory, \n",
    "                                extension=\"jpg\", \n",
    "                                csv_file=train_dataset_path, \n",
    "                                transform=data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FileNotFoundError. Maybe we slipped down a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = combined_dataset / 'combined_train.csv'\n",
    "\n",
    "train_dataset = TransformAndLoad(parent_directory=parent_directory, \n",
    "                                extension=\"jpg\", \n",
    "                                csv_file=train_dataset_path, \n",
    "                                transform=data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally!!!\n",
    "\n",
    "Now let's see what can we do with train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TransformAndLoad in module torchetl.etl object:\n",
      "\n",
      "class TransformAndLoad(torch.utils.data.dataset.Dataset)\n",
      " |  TransformAndLoad(parent_directory: str, extension: str, csv_file: str, transform: Callable = None) -> None\n",
      " |  \n",
      " |  An abstract class representing a Dataset.\n",
      " |  \n",
      " |  All other datasets should subclass it. All subclasses should override\n",
      " |  ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
      " |  supporting integer indexing in range from 0 to len(self) exclusive.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TransformAndLoad\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, idx: int) -> Tuple[numpy.ndarray, numpy.ndarray]\n",
      " |      Return the X and y of a specific instance based on the index\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      idx\n",
      " |              The index of the instance \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Tuple of X and y of a specific instance\n",
      " |  \n",
      " |  __init__(self, parent_directory: str, extension: str, csv_file: str, transform: Callable = None) -> None\n",
      " |      Class for reading csv files of train, validation, and test\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      parent_directory\n",
      " |              The parent_directory folder path. It is highly recommended to use Pathlib\n",
      " |      extension\n",
      " |              The extension we want to include in our search from the parent_directory directory\n",
      " |      csv_file\n",
      " |              The path to csv file containing X and y\n",
      " |      Transform\n",
      " |              Callable which apply transformations\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |      Return the length of the dataset\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      parent_directory\n",
      " |              The parent_directory folder path. It is highly recommended to use Pathlib\n",
      " |      extension\n",
      " |              The extension we want to include in our search from the parent_directory directory\n",
      " |      csv_file\n",
      " |              The path to csv file containing X and y\n",
      " |      Transform\n",
      " |              Callable which apply transformations\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Length of the dataset\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by using getitem method we can inspect our desired instance X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.7077,  0.7248,  0.7248,  ..., -0.0801,  0.0056,  0.0741],\n",
       "          [ 0.6906,  0.7077,  0.7077,  ..., -0.1828, -0.0629,  0.0227],\n",
       "          [ 0.6906,  0.6906,  0.6906,  ..., -0.1999, -0.0629,  0.0398],\n",
       "          ...,\n",
       "          [ 0.3823,  0.3994,  0.3994,  ..., -1.0562, -0.9192, -0.5767],\n",
       "          [ 0.3823,  0.3994,  0.3994,  ..., -1.0219, -0.7993, -0.4054],\n",
       "          [ 0.3823,  0.3823,  0.3994,  ..., -1.0048, -0.6965, -0.2684]],\n",
       " \n",
       "         [[ 0.9055,  0.9405,  0.9230,  ...,  0.1001,  0.1877,  0.2577],\n",
       "          [ 0.9055,  0.9230,  0.9055,  ...,  0.0301,  0.1176,  0.2227],\n",
       "          [ 0.9055,  0.9055,  0.9055,  ...,  0.0126,  0.1176,  0.2402],\n",
       "          ...,\n",
       "          [ 0.6954,  0.7129,  0.7129,  ..., -0.6001, -0.4951, -0.1625],\n",
       "          [ 0.6954,  0.7129,  0.7129,  ..., -0.5651, -0.3901, -0.0049],\n",
       "          [ 0.6954,  0.6954,  0.7129,  ..., -0.5476, -0.2850,  0.1352]],\n",
       " \n",
       "         [[ 1.3677,  1.3502,  1.3677,  ...,  0.5834,  0.6705,  0.7228],\n",
       "          [ 1.3328,  1.3328,  1.3328,  ...,  0.4962,  0.5834,  0.6531],\n",
       "          [ 1.3154,  1.3154,  1.3154,  ...,  0.4439,  0.5659,  0.6531],\n",
       "          ...,\n",
       "          [ 1.2108,  1.2282,  1.2282,  ...,  0.1128,  0.1825,  0.4614],\n",
       "          [ 1.2108,  1.2282,  1.2282,  ...,  0.1302,  0.2871,  0.6182],\n",
       "          [ 1.2108,  1.2108,  1.2282,  ...,  0.1476,  0.3742,  0.7576]]]), 0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the csv file, we could easily inspect our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(train_dataset_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ori/attack/hand/115/4030.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ori/attack/fixed/112/4030.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>style/attack/hand/112/9111.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>style/attack/fixed/112/4144.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>style/attack/hand/112/2045.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0  1\n",
       "0     ori/attack/hand/115/4030.jpg  0\n",
       "1    ori/attack/fixed/112/4030.jpg  0\n",
       "2   style/attack/hand/112/9111.jpg  0\n",
       "3  style/attack/fixed/112/4144.jpg  0\n",
       "4   style/attack/hand/112/2045.jpg  0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very handy. For instance we want to make sure that all that contains \"attack\" must have consistent label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      True\n",
       "1      True\n",
       "2      True\n",
       "3      True\n",
       "4      True\n",
       "5      True\n",
       "6      True\n",
       "8      True\n",
       "9      True\n",
       "10     True\n",
       "11     True\n",
       "14     True\n",
       "15     True\n",
       "16     True\n",
       "19     True\n",
       "20     True\n",
       "21     True\n",
       "23     True\n",
       "25     True\n",
       "26     True\n",
       "27     True\n",
       "28     True\n",
       "29     True\n",
       "30     True\n",
       "31     True\n",
       "32     True\n",
       "33     True\n",
       "35     True\n",
       "36     True\n",
       "37     True\n",
       "       ... \n",
       "324    True\n",
       "325    True\n",
       "326    True\n",
       "328    True\n",
       "332    True\n",
       "333    True\n",
       "334    True\n",
       "335    True\n",
       "336    True\n",
       "339    True\n",
       "341    True\n",
       "342    True\n",
       "345    True\n",
       "346    True\n",
       "348    True\n",
       "349    True\n",
       "352    True\n",
       "353    True\n",
       "355    True\n",
       "356    True\n",
       "357    True\n",
       "358    True\n",
       "359    True\n",
       "361    True\n",
       "363    True\n",
       "364    True\n",
       "365    True\n",
       "366    True\n",
       "367    True\n",
       "368    True\n",
       "Name: 0, Length: 248, dtype: bool"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[0].str.contains('attack')][0] == train_df[train_df[1] == 0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[train_df[0].str.contains('attack')][0]) == len(train_df[train_df[1] == 0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have confirmed that file that contains \"attack\" have consistent label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we have successfully create a train DataLoader. While this may looks like a long time, in practice we're saving quite a lot of time because we're not moving any files whatsoever. We also have more consistent and reproducible dataset. On top of that, debugging dataset is much easier than naive method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
