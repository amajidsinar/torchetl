{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Problem Requires Modern Solution\n",
    "\n",
    "If you're working on classification problem with your own dataset, or dataset that is available in their native format (jpg, bmp, etc) and use PyTorch as your main weapon, you'll most likely feel that the **DatasetFolder** or **ImageFolder** is not good enough. So does vanilla **torch.utils.data.Dataset**. This library attempts to bridge that gap to effectively Extract, Transform, and Load your data by extending **torch.utils.data.Dataset**.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "In the first step to train a classifier is to prepare the dataset. By default, PyTorch provides the abstraction of this process through **DatasetFolder** or **ImageFolder**. However they requires us to arrange our folder in this way\n",
    "##### root/class_x/xxx.ext\n",
    "##### root/class_x/xxy.ext\n",
    "##### root/class_x/xxz.ext\n",
    "##### root/class_y/123.ext\n",
    "##### root/class_y/nsdf3.ext\n",
    "##### root/class_y/asd932_.ext\n",
    "\n",
    "Most of the time, we don't have that, especially if we're using our own dataset (might be from scraping or a gift from someone). So we have to do something.\n",
    "\n",
    "The first, naive way is to move folders manually or using some scripts, could be python scripts, bash scripts or anything else you're comfortable with.\\n\\nIt's kind of cool until we    \n",
    "    1. Have terabytes of dataset    \n",
    "    2. Want to partition them into train, validation, and or test    \n",
    "    3. Want to explore/debug our dataset  \n",
    "    4. Want others to reproduce our results    \n",
    "    \n",
    "The second option is to make your own custom Dataset, subclassed from **torch.utils.data.Dataset**. This approach is much simpler and cleaner than the naive way, but still, we would have problem number 2 and 3    \n",
    "    2. Want to partition them into train, validation, and or test   \n",
    "    3. Want to explore/debug our dataset  \n",
    "    4. Want others to reproduce our results\n",
    "    \n",
    "This problem lead the design of ETL, a library that does Extract, Transform and Load on the fly. In a nutshell, Extract will read all the images from a parent directory, then partition those images into train, validation, and testing, and store them into csv files with the column of image path and encoded label. After that TransformAndLoad will ingest those samples efficiently to your classifier.\n",
    "\n",
    "By the way, the reason I use csv rather than txt is because pandas can parse CSV flawlessly. If we're using txt it's a little bit more complicated but maybe I'll add those feature in the future\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pt.1 Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torchetl.etl import Extract, TransformAndLoad\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jedi/Repo/GitHub/torchetl/data\n"
     ]
    }
   ],
   "source": [
    "parent_directory = Path.cwd().parent / 'data' \n",
    "print(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = Extract(parent_directory = parent_directory, \n",
    "              extension = 'jpg', \n",
    "              labels = ['attack', 'real'], \n",
    "              train_size = 0.8,\n",
    "              random_state = 69,\n",
    "              verbose = True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Extract in module torchetl.etl object:\n",
      "\n",
      "class Extract(torchetl.base.dataset.BaseDataset)\n",
      " |  Extract(parent_directory: str, extension: str, labels: List[str], train_size: float, random_state: int, verbose: bool) -> None\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Extract\n",
      " |      torchetl.base.dataset.BaseDataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, parent_directory: str, extension: str, labels: List[str], train_size: float, random_state: int, verbose: bool) -> None\n",
      " |      Class for creating csv files of train, validation, and test\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      parent_directory\n",
      " |              The parent_directory folder path. It is highly recommended to use Pathlib\n",
      " |      extension\n",
      " |              The extension we want to include in our search from the parent_directory directory\n",
      " |      labels\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  extract(self, file_prefix: str, save_path: str, is_random_sampling: bool)\n",
      " |      Create csv file of train, validation, and test\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      file_prefix\n",
      " |              The prefix of train, validation, and test file_prefix\n",
      " |              Have the format of file_prefix_train.csv, file_prefix_validation.csv, and test_validation.csv\n",
      " |      save_path\n",
      " |              The parent_directory folder name of file_prefix_train.csv, file_prefix_validation.csv, and test_validation.csv\n",
      " |      is_random_sampling\n",
      " |              extract train, validation, and test based on random sampling. If set to false, then stratify sampling is applied.\n",
      " |              The best practice is to use stratify sampling for classification tasks and random sampling for regression tasks\n",
      " |      Returns\n",
      " |      -------\n",
      " |      train, validation, and test csv with the following name:\n",
      " |      file_prefix_train.csv, file_prefix_validation.csv, and test_validation.csv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torchetl.base.dataset.BaseDataset:\n",
      " |  \n",
      " |  read_files(self) -> Iterable\n",
      " |      Return parent_directory directory\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      origin\n",
      " |              the path to read files\n",
      " |  \n",
      " |  read_parent_directory(self) -> str\n",
      " |      Return parent_directory directory\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  show_files(self, n: int = None) -> Sequence\n",
      " |      Show files to consume\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      number_of_files_to_show\n",
      " |              number of files to show\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      origin\n",
      " |              the path to read files\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torchetl.base.dataset.BaseDataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(combined_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we know that Extract inherits show_files method from BaseDataset. Let's print the first 5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/ori/attack/hand/112/4187.jpg'),\n",
       " PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/ori/attack/hand/112/10099.jpg'),\n",
       " PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/ori/attack/hand/112/3159.jpg'),\n",
       " PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/ori/attack/hand/112/6021.jpg'),\n",
       " PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/ori/attack/hand/112/1028.jpg')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset.show_files(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that everything went perfect, it's time to save them into our desired path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/jedi/Repo/GitHub/torchetl/data/combined')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = Path.cwd().parent / \"data\" / \"combined\"\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating whole dataset array\n",
      "Finished splitting dataset into train, validation, and test\n",
      "Finished writing combined_train.csv into /Users/jedi/Repo/GitHub/torchetl/data/combined\n",
      "Finished writing combined_validation.csv into /Users/jedi/Repo/GitHub/torchetl/data/combined\n",
      "Finished writing combined_test.csv into /Users/jedi/Repo/GitHub/torchetl/data/combined\n"
     ]
    }
   ],
   "source": [
    "combined_dataset.extract(file_prefix=\"combined\", save_path=save_path, is_random_sampling=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pt. 2 Transform and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = Path.cwd() / 'data' / 'combined'\n",
    "train_dataset_path = combined_dataset / 'combined_trains.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "transforms.ToPILImage(),\n",
    "transforms.RandomResizedCrop(224),\n",
    "transforms.RandomHorizontalFlip(),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jedi/Repo/GitHub/torchetl/tutorial/data/combined/combined_trains.csv does not exist\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TransformAndLoad(parent_directory=parent_directory, \n",
    "                                extension=\"jpg\", \n",
    "                                csv_file=train_dataset_path, \n",
    "                                transform=data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FileNotFoundError. Maybe we slipped down a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jedi/Repo/GitHub/torchetl/tutorial/data/combined/combined_train.csv does not exist\n"
     ]
    }
   ],
   "source": [
    "train_dataset_path = combined_dataset / 'combined_train.csv'\n",
    "\n",
    "train_dataset = TransformAndLoad(parent_directory=parent_directory, \n",
    "                                extension=\"jpg\", \n",
    "                                csv_file=train_dataset_path, \n",
    "                                transform=data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally!!!\n",
    "\n",
    "Now let's see what can we do with train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TransformAndLoad in module torchetl.etl object:\n",
      "\n",
      "class TransformAndLoad(torch.utils.data.dataset.Dataset)\n",
      " |  TransformAndLoad(parent_directory: str, extension: str, csv_file: str, transform: Callable = None) -> None\n",
      " |  \n",
      " |  An abstract class representing a Dataset.\n",
      " |  \n",
      " |  All other datasets should subclass it. All subclasses should override\n",
      " |  ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
      " |  supporting integer indexing in range from 0 to len(self) exclusive.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TransformAndLoad\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, idx: int) -> Tuple[numpy.ndarray, numpy.ndarray]\n",
      " |      Return the X and y of a specific instance based on the index\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      idx\n",
      " |              The index of the instance \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Tuple of X and y of a specific instance\n",
      " |  \n",
      " |  __init__(self, parent_directory: str, extension: str, csv_file: str, transform: Callable = None) -> None\n",
      " |      Class for reading csv files of train, validation, and test\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      parent_directory\n",
      " |              The parent_directory folder path. It is highly recommended to use Pathlib\n",
      " |      extension\n",
      " |              The extension we want to include in our search from the parent_directory directory\n",
      " |      csv_file\n",
      " |              The path to csv file containing X and y\n",
      " |      Transform\n",
      " |              Callable which apply transformations\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |      Return the length of the dataset\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      parent_directory\n",
      " |              The parent_directory folder path. It is highly recommended to use Pathlib\n",
      " |      extension\n",
      " |              The extension we want to include in our search from the parent_directory directory\n",
      " |      csv_file\n",
      " |              The path to csv file containing X and y\n",
      " |      Transform\n",
      " |              Callable which apply transformations\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Length of the dataset\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by using getitem method we can inspect our desired instance X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformAndLoad' object has no attribute 'csv_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-df7b97b9600b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/torchetl/lib/python3.7/site-packages/torchetl/etl.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0mTuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \t\t\"\"\"\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0mparent_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_directory\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0mparent_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransformAndLoad' object has no attribute 'csv_file'"
     ]
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the csv file, we could easily inspect our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(train_dataset_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very handy. For instance we want to make sure that all that contains \"attack\" must have consistent label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[0].str.contains('attack')][0] == train_df[train_df[1] == 0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df[train_df[0].str.contains('attack')][0]) == len(train_df[train_df[1] == 0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have confirmed that file that contains \"attack\" have consistent label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we have successfully create a train DataLoader. While this may looks like a long time, in practice we're saving quite a lot of time because we're not moving any files whatsoever. We also have more consistent and reproducible dataset. On top of that, debugging dataset is much easier than naive method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
